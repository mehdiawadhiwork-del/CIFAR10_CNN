# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gU6rDdY9G5iwPggV7VumChB28IYnB9cv
"""

# ================================================================
# 1. Importation des librairies nécessaires
# ================================================================
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import numpy as np

# Vérification de la version
print("Version TensorFlow :", tf.__version__)

# ================================================================
# 2. Chargement de la base de données CIFAR-10
# ================================================================
# Le dataset CIFAR-10 contient 60 000 images (32x32 pixels, 3 canaux de couleur)
# Réparties en 10 classes (avion, voiture, oiseau, chat, etc.)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

print("Taille du jeu d'entraînement :", x_train.shape)
print("Taille du jeu de test :", x_test.shape)

# ================================================================
# 3. Prétraitement des données
# ================================================================
# Normalisation des valeurs de pixels entre 0 et 1
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Les étiquettes sont déjà des entiers entre 0 et 9
num_classes = 10

# Pour visualiser quelques exemples
class_names = ['avion', 'auto', 'oiseau', 'chat', 'cerf',
               'chien', 'grenouille', 'cheval', 'bateau', 'camion']

plt.figure(figsize=(10, 3))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(x_train[i])
    plt.title(class_names[y_train[i][0]])
    plt.axis("off")
plt.show()

# ================================================================
# 4. Création du modèle CNN
# ================================================================
# Architecture :
# - 3 blocs convolution + max pooling
# - Couches denses à la fin pour la classification

model = models.Sequential([
    # Bloc 1
    layers.Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(32,32,3)),
    layers.Conv2D(32, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    # Bloc 2
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.Conv2D(64, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    # Bloc 3
    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.Conv2D(128, (3,3), activation='relu', padding='same'),
    layers.MaxPooling2D((2,2)),
    layers.Dropout(0.25),

    # Couches entièrement connectées
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation='softmax')
])

model.summary()

# ================================================================
# 5. Compilation du modèle
# ================================================================
# Choix :
# - Optimiseur : Adam (bon équilibre vitesse/stabilité)
# - Fonction de perte : sparse_categorical_crossentropy
# - Métrique : accuracy

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# ================================================================
# 6. Entraînement du modèle
# ================================================================
# epochs = 30, batch_size = 64

history = model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=64,
    validation_split=0.2,
    verbose=1
)

# ================================================================
# 7. Évaluation sur le jeu de test
# ================================================================
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"Précision sur le jeu de test : {test_acc * 100:.2f}%")
print(f"Perte sur le jeu de test : {test_loss:.4f}")

# ================================================================
# 8. Visualisation des courbes d'apprentissage
# ================================================================
plt.figure(figsize=(12,5))

# Précision
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Entraînement')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title("Évolution de la précision")
plt.xlabel("Époque")
plt.ylabel("Accuracy")
plt.legend()

# Perte
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Entraînement')
plt.plot(history.history['val_loss'], label='Validation')
plt.title("Évolution de la perte")
plt.xlabel("Époque")
plt.ylabel("Loss")
plt.legend()

plt.show()

# ================================================================
# 10. Résumé et discussion des hyperparamètres
# ================================================================

"""
Résumé du modèle :
------------------
- Type : Réseau de neurones convolutionnel (CNN)
- Nombre de couches : 3 blocs convolutionnels + 2 couches denses
- Nombre total de paramètres : ≈ 3 millions
- Fonctions d’activation : ReLU (rapide et efficace)
- Sortie : Softmax (10 classes)

Choix des hyperparamètres :
----------------------------
- Optimiseur : Adam (vitesse et stabilité)
- Fonction de perte : sparse_categorical_crossentropy
- Batch size : 64 (compromis mémoire/vitesse)
- Époques : 30
- Dropout : entre 0.25 et 0.5 pour éviter le surapprentissage
- Padding = "same" pour conserver les dimensions des images

Performances typiques :
------------------------
- Précision d’entraînement : ~90–95 %6
- Précision de test : ~80–85 %
  (selon le nombre d’époques et les ressources disponibles)

Commentaires :
---------------
Ce CNN apprend efficacement les motifs visuels de CIFAR-10.
Il surpasse largement le MLP car il exploite la structure spatiale
des images grâce aux convolutions et au pooling.
"""